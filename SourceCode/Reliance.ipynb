{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reliance",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJFI2rBFwWro",
        "outputId": "698c7a08-7338-4bc7-8b7f-4ca58c85d319"
      },
      "source": [
        "pip install yfinanace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement yfinanace (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for yfinanace\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G2VCG6mnH_O"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "plt.style.use('fivethirtyeight')\n",
        "import pandas_datareader as web\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2RWUcysmCF4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "cb5bd7e6-e482-4edc-e612-a669dcc2975f"
      },
      "source": [
        "yf.pdr_override()\n",
        "df = pdr.get_data_yahoo(\"COFORGE\", start=\"2016-01-01\", end=pd.datetime.now().date())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f8e711fe7da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdr_override\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_yahoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"COFORGE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2016-01-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'yf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fVdWD4vjdfg"
      },
      "source": [
        "#CREATE a new dataframe with only close column\n",
        "data = df.filter(['Close'])\n",
        "#convert the df to numpy array\n",
        "dataset = data.values\n",
        "#get the nmber of rows to train the model on\n",
        "training_data_len = math.ceil(len(dataset) *0.85)\n",
        "\n",
        "\n",
        "#scale the data\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "\n",
        "#create the training dataset\n",
        "#create the scaled training dataset\n",
        "train_data = scaled_data[0:training_data_len, :]\n",
        "#split the data into x_train and y_train data sets\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(60, len(train_data)):\n",
        "  x_train.append(train_data[i-60:i, 0])\n",
        "  y_train.append(train_data[i, 0])\n",
        "#   if i<=61:\n",
        "#     print(x_train)\n",
        "#     print(y_train)\n",
        "#     print()\n",
        "\n",
        "\n",
        "\n",
        "#convert the x_train and y_train to numpy arrays\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "\n",
        "\n",
        "\n",
        "#reshape the data\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " #build lstm model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(25))\n",
        "model.add(Dense(1))\n",
        "\n",
        "#compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "#train the model\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=50)   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#create the testing data set\n",
        "#create new array contiang scaled value from index  1940 to 2857\n",
        "test_data = scaled_data[training_data_len - 60: , :]\n",
        "#create the datasets x_test and y_test\n",
        "x_test = []\n",
        "y_test = dataset[training_data_len:, :]\n",
        "for i in range(60, len(test_data)):\n",
        "  x_test.append(test_data[i-60:i, 0])\n",
        "\n",
        "\n",
        "#convert the data to  a numpy array\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "#reshape the data\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "#get the model's predited price values\n",
        "predictions = model.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "\n",
        "#create a new dataframe\n",
        "new_df = df.filter(['Close'])\n",
        "#get the last 60 day closing price values and convert the dataframe to an array\n",
        "last_60_days = new_df[-60:].values\n",
        "#scale the data to be values btw 0 and 1\n",
        "last_60_days_scaled = scaler.transform(last_60_days)\n",
        "#create an empty list\n",
        "X_test = []\n",
        "#append past 60 days to X_test\n",
        "X_test.append(last_60_days_scaled)\n",
        "#convert the x_test data set to a numpy array\n",
        "X_test = np.array(X_test)\n",
        "#reshape\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "#get the predicted scaled price\n",
        "pred_price = model.predict(X_test)\n",
        "#undo the scaling\n",
        "pred_price = scaler.inverse_transform(pred_price)\n",
        "\n",
        "print(pred_price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sICyUDeEXDh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4M9lbYB9Uh2"
      },
      "source": [
        "rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDaQ91CR9Ypu"
      },
      "source": [
        "valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XlcvbKJypNk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJjP7rwByux9"
      },
      "source": [
        "# from flask import Flask\n",
        "# from flask_ngrok import run_with_ngrok\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import math\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, LSTM\n",
        "# from pandas_datareader import data as pdr\n",
        "# import yfinance as yf\n",
        "# import datetime\n",
        "# plt.style.use('fivethirtyeight')\n",
        "# import pandas_datareader as web\n",
        "\n",
        "\n",
        "# app = Flask(__name__)\n",
        "# run_with_ngrok(app)   \n",
        "\n",
        "# @app.route(\"/\")\n",
        "# def test():\n",
        "#   return \"Working, Yay!!!\"  \n",
        "\n",
        "# @app.route(\"/test\")\n",
        "# def home():\n",
        "\n",
        "#   yf.pdr_override()\n",
        "#   df = pdr.get_data_yahoo(\"RELIANCE.NS\", start=\"2000-01-01\", end=pd.datetime.now().date())\n",
        "\n",
        "#   data = df.filter(['Close'])\n",
        "#   dataset = data.values\n",
        "#   training_data_len = math.ceil(len(dataset) *0.85)\n",
        "\n",
        "#   scaler = MinMaxScaler(feature_range=(0,1))\n",
        "#   scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "#   train_data = scaled_data[0:training_data_len, :]\n",
        "  \n",
        "#   x_train = []\n",
        "#   y_train = []\n",
        "\n",
        "#   for i in range(60, len(train_data)):\n",
        "#     x_train.append(train_data[i-60:i, 0])\n",
        "#     y_train.append(train_data[i, 0])\n",
        "\n",
        "#   x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "#   x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "\n",
        "#   model = Sequential()\n",
        "#   model.add(LSTM(50, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
        "#   model.add(LSTM(50, return_sequences=False))\n",
        "#   model.add(Dense(25))\n",
        "#   model.add(Dense(1))\n",
        "\n",
        "#   model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "#   model.fit(x_train, y_train, batch_size=32, epochs=1)   \n",
        "#   test_data = scaled_data[training_data_len - 60: , :]\n",
        "  \n",
        "#   x_test = []\n",
        "#   y_test = dataset[training_data_len:, :]\n",
        "#   for i in range(60, len(test_data)):\n",
        "#     x_test.append(test_data[i-60:i, 0])\n",
        "\n",
        "\n",
        "#   x_test = np.array(x_test)\n",
        "#   x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "#   predictions = model.predict(x_test)\n",
        "#   predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "#   train = data[:training_data_len]\n",
        "#   valid = data[training_data_len:]\n",
        "#   valid['Predictions'] = predictions\n",
        "\n",
        "#   new_df = df.filter(['Close'])\n",
        "#   last_60_days = new_df[-60:].values\n",
        "\n",
        "#   last_60_days_scaled = scaler.transform(last_60_days)\n",
        "\n",
        "#   X_test = []\n",
        "#   X_test.append(last_60_days_scaled)\n",
        "#   X_test = np.array(X_test)\n",
        "#   X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "#   pred_price = model.predict(X_test)\n",
        "#   pred_price = scaler.inverse_transform(pred_price)\n",
        "#   print(pred_price)\n",
        "#   return str(pred_price)\n",
        "    \n",
        "# app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDQ31V2OxzTv"
      },
      "source": [
        "# ruko jara sabar karo\n",
        "yf.pdr_override()\n",
        "df = pdr.get_data_yahoo(\"RELIANCE.NS\", start=\"2000-01-01\", end=pd.datetime.now().date())\n",
        "\n",
        "#CREATE a new dataframe with only close column\n",
        "data = df.filter(['Close'])\n",
        "#convert the df to numpy array\n",
        "dataset = data.values\n",
        "#get the nmber of rows to train the model on\n",
        "training_data_len = math.ceil(len(dataset) *0.85)\n",
        "\n",
        "\n",
        "#scale the data\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "\n",
        "#create the training dataset\n",
        "#create the scaled training dataset\n",
        "train_data = scaled_data[0:training_data_len, :]\n",
        "#split the data into x_train and y_train data sets\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(60, len(train_data)):\n",
        "  x_train.append(train_data[i-60:i, 0])\n",
        "  y_train.append(train_data[i, 0])\n",
        "\n",
        "#convert the x_train and y_train to numpy arrays\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "\n",
        "#reshape the data\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "\n",
        "#build lstm model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(25))\n",
        "model.add(Dense(1))\n",
        "\n",
        "#compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "#train the model\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=50)   \n",
        "\n",
        "#create the testing data set\n",
        "#create new array contiang scaled value from index  1940 to 2857\n",
        "test_data = scaled_data[training_data_len - 60: , :]\n",
        "#create the datasets x_test and y_test\n",
        "x_test = []\n",
        "y_test = dataset[training_data_len:, :]\n",
        "for i in range(60, len(test_data)):\n",
        "  x_test.append(test_data[i-60:i, 0])\n",
        "\n",
        "\n",
        "#convert the data to  a numpy array\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "#reshape the data\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "#get the model's predited price values\n",
        "predictions = model.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "#plot the data\n",
        "train = data[:training_data_len]\n",
        "valid = data[training_data_len:]\n",
        "valid['Predictions'] = predictions\n",
        "\n",
        "\n",
        "new_df = df.filter(['Close'])\n",
        "\n",
        "last_60_days = new_df[-60:].values\n",
        "\n",
        "last_60_days_scaled = scaler.transform(last_60_days)\n",
        "\n",
        "X_test = []\n",
        "\n",
        "X_test.append(last_60_days_scaled)\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "pred_price = model.predict(X_test)\n",
        "\n",
        "pred_price = scaler.inverse_transform(pred_price)\n",
        "\n",
        "print(pred_price)\n",
        "# rmse = np.sqrt(np.mean(predictions - y_test)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNXexf__x-fy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}